{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question Answering with LangChain, OpenAI, and MultiQuery Retriever\n",
    "\n",
    "This interactive workbook demonstrates example of Elasticsearch's [MultiQuery Retriever](https://api.python.langchain.com/en/latest/retrievers/langchain.retrievers.multi_query.MultiQueryRetriever.html) to generate similar queries for a given user input and apply all queries to retrieve a larger set of relevant documents from a vectorstore.\n",
    "\n",
    "Before we begin, we first split the fictional workplace documents into passages with `langchain` and uses OpenAI to transform these passages into embeddings and then store these into Elasticsearch.\n",
    "\n",
    "We will then ask a question, generate similar questions using langchain and OpenAI, retrieve relevant passages from the vector store, and use langchain and OpenAI again to provide a summary for the questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install packages and import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: langchain[elasticsearch] in c:\\users\\huawei\\anaconda3\\lib\\site-packages (0.3.23)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.51 in c:\\users\\huawei\\anaconda3\\lib\\site-packages (from langchain[elasticsearch]) (0.3.52)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in c:\\users\\huawei\\anaconda3\\lib\\site-packages (from langchain[elasticsearch]) (0.3.8)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.17 in c:\\users\\huawei\\anaconda3\\lib\\site-packages (from langchain[elasticsearch]) (0.3.13)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\users\\huawei\\anaconda3\\lib\\site-packages (from langchain[elasticsearch]) (2.8.2)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\huawei\\anaconda3\\lib\\site-packages (from langchain[elasticsearch]) (2.0.34)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\huawei\\anaconda3\\lib\\site-packages (from langchain[elasticsearch]) (2.32.3)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\huawei\\anaconda3\\lib\\site-packages (from langchain[elasticsearch]) (6.0.1)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\users\\huawei\\anaconda3\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.51->langchain[elasticsearch]) (8.2.3)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\huawei\\anaconda3\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.51->langchain[elasticsearch]) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in c:\\users\\huawei\\anaconda3\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.51->langchain[elasticsearch]) (24.1)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in c:\\users\\huawei\\anaconda3\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.51->langchain[elasticsearch]) (4.11.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\huawei\\anaconda3\\lib\\site-packages (from langsmith<0.4,>=0.1.17->langchain[elasticsearch]) (0.27.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\huawei\\anaconda3\\lib\\site-packages (from langsmith<0.4,>=0.1.17->langchain[elasticsearch]) (3.10.15)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\users\\huawei\\anaconda3\\lib\\site-packages (from langsmith<0.4,>=0.1.17->langchain[elasticsearch]) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in c:\\users\\huawei\\anaconda3\\lib\\site-packages (from langsmith<0.4,>=0.1.17->langchain[elasticsearch]) (0.23.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\huawei\\anaconda3\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain[elasticsearch]) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in c:\\users\\huawei\\anaconda3\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain[elasticsearch]) (2.20.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\huawei\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain[elasticsearch]) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\huawei\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain[elasticsearch]) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\huawei\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain[elasticsearch]) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\huawei\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain[elasticsearch]) (2025.1.31)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\huawei\\anaconda3\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain[elasticsearch]) (3.0.1)\n",
      "Requirement already satisfied: anyio in c:\\users\\huawei\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain[elasticsearch]) (4.2.0)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\huawei\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain[elasticsearch]) (1.0.2)\n",
      "Requirement already satisfied: sniffio in c:\\users\\huawei\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain[elasticsearch]) (1.3.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\huawei\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain[elasticsearch]) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\huawei\\anaconda3\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.51->langchain[elasticsearch]) (2.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~eras (c:\\Users\\Huawei\\anaconda3\\Lib\\site-packages)\n",
      "WARNING: langchain 0.3.23 does not provide the extra 'elasticsearch'\n",
      "WARNING: Ignoring invalid distribution ~eras (c:\\Users\\Huawei\\anaconda3\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~eras (c:\\Users\\Huawei\\anaconda3\\Lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "pip install -U langchain[elasticsearch]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain-elasticsearch\n",
      "  Downloading langchain_elasticsearch-0.3.2-py3-none-any.whl.metadata (8.3 kB)\n",
      "Collecting elasticsearch<9.0.0,>=8.13.1 (from elasticsearch[vectorstore-mmr]<9.0.0,>=8.13.1->langchain-elasticsearch)\n",
      "  Downloading elasticsearch-8.18.0-py3-none-any.whl.metadata (9.2 kB)\n",
      "Requirement already satisfied: langchain-core<0.4.0,>=0.3.0 in c:\\users\\huawei\\anaconda3\\lib\\site-packages (from langchain-elasticsearch) (0.3.52)\n",
      "Collecting elastic-transport<9,>=8.15.1 (from elasticsearch<9.0.0,>=8.13.1->elasticsearch[vectorstore-mmr]<9.0.0,>=8.13.1->langchain-elasticsearch)\n",
      "  Downloading elastic_transport-8.17.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: python-dateutil in c:\\users\\huawei\\anaconda3\\lib\\site-packages (from elasticsearch<9.0.0,>=8.13.1->elasticsearch[vectorstore-mmr]<9.0.0,>=8.13.1->langchain-elasticsearch) (2.9.0.post0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\huawei\\anaconda3\\lib\\site-packages (from elasticsearch<9.0.0,>=8.13.1->elasticsearch[vectorstore-mmr]<9.0.0,>=8.13.1->langchain-elasticsearch) (4.11.0)\n",
      "Requirement already satisfied: numpy>=1 in c:\\users\\huawei\\anaconda3\\lib\\site-packages (from elasticsearch[vectorstore-mmr]<9.0.0,>=8.13.1->langchain-elasticsearch) (2.2.4)\n",
      "Collecting simsimd>=3 (from elasticsearch[vectorstore-mmr]<9.0.0,>=8.13.1->langchain-elasticsearch)\n",
      "  Downloading simsimd-6.2.1-cp312-cp312-win_amd64.whl.metadata (67 kB)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.125 in c:\\users\\huawei\\anaconda3\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.0->langchain-elasticsearch) (0.3.13)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\users\\huawei\\anaconda3\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.0->langchain-elasticsearch) (8.2.3)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\huawei\\anaconda3\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.0->langchain-elasticsearch) (1.33)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\huawei\\anaconda3\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.0->langchain-elasticsearch) (6.0.1)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in c:\\users\\huawei\\anaconda3\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.0->langchain-elasticsearch) (24.1)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\users\\huawei\\anaconda3\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.0->langchain-elasticsearch) (2.8.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.26.2 in c:\\users\\huawei\\anaconda3\\lib\\site-packages (from elastic-transport<9,>=8.15.1->elasticsearch<9.0.0,>=8.13.1->elasticsearch[vectorstore-mmr]<9.0.0,>=8.13.1->langchain-elasticsearch) (2.2.3)\n",
      "Requirement already satisfied: certifi in c:\\users\\huawei\\anaconda3\\lib\\site-packages (from elastic-transport<9,>=8.15.1->elasticsearch<9.0.0,>=8.13.1->elasticsearch[vectorstore-mmr]<9.0.0,>=8.13.1->langchain-elasticsearch) (2025.1.31)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\huawei\\anaconda3\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.0->langchain-elasticsearch) (2.1)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\huawei\\anaconda3\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.0->langchain-elasticsearch) (0.27.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\huawei\\anaconda3\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.0->langchain-elasticsearch) (3.10.15)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\huawei\\anaconda3\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.0->langchain-elasticsearch) (2.32.3)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\users\\huawei\\anaconda3\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.0->langchain-elasticsearch) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in c:\\users\\huawei\\anaconda3\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.0->langchain-elasticsearch) (0.23.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\huawei\\anaconda3\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<0.4.0,>=0.3.0->langchain-elasticsearch) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in c:\\users\\huawei\\anaconda3\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<0.4.0,>=0.3.0->langchain-elasticsearch) (2.20.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\huawei\\anaconda3\\lib\\site-packages (from python-dateutil->elasticsearch<9.0.0,>=8.13.1->elasticsearch[vectorstore-mmr]<9.0.0,>=8.13.1->langchain-elasticsearch) (1.16.0)\n",
      "Requirement already satisfied: anyio in c:\\users\\huawei\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.0->langchain-elasticsearch) (4.2.0)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\huawei\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.0->langchain-elasticsearch) (1.0.2)\n",
      "Requirement already satisfied: idna in c:\\users\\huawei\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.0->langchain-elasticsearch) (3.7)\n",
      "Requirement already satisfied: sniffio in c:\\users\\huawei\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.0->langchain-elasticsearch) (1.3.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\huawei\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.0->langchain-elasticsearch) (0.14.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\huawei\\anaconda3\\lib\\site-packages (from requests<3,>=2->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.0->langchain-elasticsearch) (3.3.2)\n",
      "Downloading langchain_elasticsearch-0.3.2-py3-none-any.whl (45 kB)\n",
      "Downloading elasticsearch-8.18.0-py3-none-any.whl (895 kB)\n",
      "   ---------------------------------------- 0.0/895.2 kB ? eta -:--:--\n",
      "   ---------------------------------------- 895.2/895.2 kB 8.1 MB/s eta 0:00:00\n",
      "Downloading elastic_transport-8.17.1-py3-none-any.whl (64 kB)\n",
      "Downloading simsimd-6.2.1-cp312-cp312-win_amd64.whl (87 kB)\n",
      "Installing collected packages: simsimd, elastic-transport, elasticsearch, langchain-elasticsearch\n",
      "Successfully installed elastic-transport-8.17.1 elasticsearch-8.18.0 langchain-elasticsearch-0.3.2 simsimd-6.2.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~eras (c:\\Users\\Huawei\\anaconda3\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~eras (c:\\Users\\Huawei\\anaconda3\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~eras (c:\\Users\\Huawei\\anaconda3\\Lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "pip install -U langchain-elasticsearch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numpy<2Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "  Downloading numpy-1.26.4-cp312-cp312-win_amd64.whl.metadata (61 kB)\n",
      "Downloading numpy-1.26.4-cp312-cp312-win_amd64.whl (15.5 MB)\n",
      "   ---------------------------------------- 0.0/15.5 MB ? eta -:--:--\n",
      "   ------ --------------------------------- 2.4/15.5 MB 14.9 MB/s eta 0:00:01\n",
      "   -------------- ------------------------- 5.8/15.5 MB 16.8 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 9.4/15.5 MB 15.9 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 12.3/15.5 MB 16.1 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 14.9/15.5 MB 14.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 15.5/15.5 MB 13.6 MB/s eta 0:00:00\n",
      "Installing collected packages: numpy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.2.4\n",
      "    Uninstalling numpy-2.2.4:\n",
      "      Successfully uninstalled numpy-2.2.4\n",
      "Successfully installed numpy-1.26.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~eras (c:\\Users\\Huawei\\anaconda3\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~eras (c:\\Users\\Huawei\\anaconda3\\Lib\\site-packages)\n",
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\Huawei\\anaconda3\\Lib\\site-packages\\~umpy.libs'.\n",
      "  You can safely remove it manually.\n",
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\Huawei\\anaconda3\\Lib\\site-packages\\~umpy'.\n",
      "  You can safely remove it manually.\n",
      "WARNING: Ignoring invalid distribution ~eras (c:\\Users\\Huawei\\anaconda3\\Lib\\site-packages)\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "jupyter-bokeh 4.0.5 requires ipywidgets==8.*, but you have ipywidgets 7.7.5 which is incompatible.\n",
      "tensorflow-intel 2.18.0 requires keras>=3.5.0, but you have keras 2.14.0 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "pip install \"numpy<2\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python was not found; run without arguments to install from the Microsoft Store, or disable this shortcut from Settings > Apps > Advanced app settings > App execution aliases.\n"
     ]
    }
   ],
   "source": [
    "!python3 -m pip install -qU jq lark langchain langchain-elasticsearch langchain_openai tiktoken\n",
    "\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "from langchain_elasticsearch import ElasticsearchStore\n",
    "from langchain_openai.llms import OpenAI\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "from getpass import getpass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to Elasticsearch\n",
    "\n",
    "ℹ️ We're using an Elastic Cloud deployment of Elasticsearch for this notebook. If you don't have an Elastic Cloud deployment, sign up [here](https://cloud.elastic.co/registration?utm_source=github&utm_content=elasticsearch-labs-notebook) for a free trial. \n",
    "\n",
    "We'll use the **Cloud ID** to identify our deployment, because we are using Elastic Cloud deployment. To find the Cloud ID for your deployment, go to https://cloud.elastic.co/deployments and select your deployment.\n",
    "\n",
    "We will use [ElasticsearchStore](https://api.python.langchain.com/en/latest/vectorstores/langchain.vectorstores.elasticsearch.ElasticsearchStore.html) to connect to our elastic cloud deployment, This would help create and index data easily.  We would also send list of documents that we created in the previous step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Please provide either elasticsearch_url or cloud_id.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 12\u001b[0m\n\u001b[0;32m      8\u001b[0m OPENAI_API_KEY \u001b[38;5;241m=\u001b[39m getpass(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOpenAI API key: \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     10\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m OpenAIEmbeddings(openai_api_key\u001b[38;5;241m=\u001b[39mOPENAI_API_KEY)\n\u001b[1;32m---> 12\u001b[0m vectorstore \u001b[38;5;241m=\u001b[39m ElasticsearchStore(\n\u001b[0;32m     13\u001b[0m     es_cloud_id\u001b[38;5;241m=\u001b[39mELASTIC_CLOUD_ID,\n\u001b[0;32m     14\u001b[0m     es_api_key\u001b[38;5;241m=\u001b[39mELASTIC_API_KEY,\n\u001b[0;32m     15\u001b[0m     index_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myour-index-name-here\u001b[39m\u001b[38;5;124m\"\u001b[39m,  \u001b[38;5;66;03m# Replace with something like \"my_docs_index\"\u001b[39;00m\n\u001b[0;32m     16\u001b[0m     embedding\u001b[38;5;241m=\u001b[39membeddings,\n\u001b[0;32m     17\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Huawei\\anaconda3\\Lib\\site-packages\\langchain_elasticsearch\\_sync\\vectorstores.py:334\u001b[0m, in \u001b[0;36mElasticsearchStore.__init__\u001b[1;34m(self, index_name, embedding, es_connection, es_url, es_cloud_id, es_user, es_api_key, es_password, vector_query_field, query_field, distance_strategy, strategy, es_params, custom_index_settings)\u001b[0m\n\u001b[0;32m    331\u001b[0m     embedding_service \u001b[38;5;241m=\u001b[39m EmbeddingServiceAdapter(embedding)\n\u001b[0;32m    333\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m es_connection:\n\u001b[1;32m--> 334\u001b[0m     es_connection \u001b[38;5;241m=\u001b[39m create_elasticsearch_client(\n\u001b[0;32m    335\u001b[0m         url\u001b[38;5;241m=\u001b[39mes_url,\n\u001b[0;32m    336\u001b[0m         cloud_id\u001b[38;5;241m=\u001b[39mes_cloud_id,\n\u001b[0;32m    337\u001b[0m         api_key\u001b[38;5;241m=\u001b[39mes_api_key,\n\u001b[0;32m    338\u001b[0m         username\u001b[38;5;241m=\u001b[39mes_user,\n\u001b[0;32m    339\u001b[0m         password\u001b[38;5;241m=\u001b[39mes_password,\n\u001b[0;32m    340\u001b[0m         params\u001b[38;5;241m=\u001b[39mes_params,\n\u001b[0;32m    341\u001b[0m     )\n\u001b[0;32m    343\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_store \u001b[38;5;241m=\u001b[39m EVectorStore(\n\u001b[0;32m    344\u001b[0m     client\u001b[38;5;241m=\u001b[39mes_connection,\n\u001b[0;32m    345\u001b[0m     index\u001b[38;5;241m=\u001b[39mindex_name,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    351\u001b[0m     custom_index_settings\u001b[38;5;241m=\u001b[39mcustom_index_settings,\n\u001b[0;32m    352\u001b[0m )\n\u001b[0;32m    354\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding \u001b[38;5;241m=\u001b[39m embedding\n",
      "File \u001b[1;32mc:\\Users\\Huawei\\anaconda3\\Lib\\site-packages\\langchain_elasticsearch\\client.py:26\u001b[0m, in \u001b[0;36mcreate_elasticsearch_client\u001b[1;34m(url, cloud_id, api_key, username, password, params)\u001b[0m\n\u001b[0;32m     24\u001b[0m     connection_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcloud_id\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m cloud_id\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 26\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease provide either elasticsearch_url or cloud_id.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m api_key:\n\u001b[0;32m     29\u001b[0m     connection_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapi_key\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m api_key\n",
      "\u001b[1;31mValueError\u001b[0m: Please provide either elasticsearch_url or cloud_id."
     ]
    }
   ],
   "source": [
    "# https://www.elastic.co/search-labs/tutorials/install-elasticsearch/elastic-cloud#finding-your-cloud-id\n",
    "ELASTIC_CLOUD_ID = getpass(\"Elastic Cloud ID: \")\n",
    "\n",
    "# https://www.elastic.co/search-labs/tutorials/install-elasticsearch/elastic-cloud#creating-an-api-key\n",
    "ELASTIC_API_KEY = getpass(\"Elastic Api Key: \")\n",
    "\n",
    "# https://platform.openai.com/api-keys\n",
    "OPENAI_API_KEY = getpass(\"OpenAI API key: \")\n",
    "\n",
    "embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)\n",
    "\n",
    "vectorstore = ElasticsearchStore(\n",
    "    es_cloud_id=ELASTIC_CLOUD_ID,\n",
    "    es_api_key=ELASTIC_API_KEY,\n",
    "    index_name=\"your-index-name-here\",  # Replace with something like \"my_docs_index\"\n",
    "    embedding=embeddings,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexing Data into Elasticsearch\n",
    "Let's download the sample dataset and deserialize the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "import json\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/elastic/elasticsearch-labs/main/example-apps/chatbot-rag-app/data/data.json\"\n",
    "\n",
    "response = urlopen(url)\n",
    "data = json.load(response)\n",
    "\n",
    "with open(\"temp.json\", \"w\") as json_file:\n",
    "    json.dump(data, json_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Documents into Passages\n",
    "\n",
    "We’ll chunk documents into passages in order to improve the retrieval specificity and to ensure that we can provide multiple passages within the context window of the final question answering prompt.\n",
    "\n",
    "Here we are chunking documents into 800 token passages with an overlap of 400 tokens.\n",
    "\n",
    "Here we are using a simple splitter but Langchain offers more advanced splitters to reduce the chance of context being lost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "Module langchain_community.document_loaders not found. Please install langchain-community to access this module. You can install it using `pip install -U langchain-community`",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\Huawei\\anaconda3\\Lib\\site-packages\\langchain\\_api\\module_import.py:69\u001b[0m, in \u001b[0;36mcreate_importer.<locals>.import_by_name\u001b[1;34m(name)\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 69\u001b[0m     module \u001b[38;5;241m=\u001b[39m importlib\u001b[38;5;241m.\u001b[39mimport_module(new_module)\n\u001b[0;32m     70\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\Huawei\\anaconda3\\Lib\\importlib\\__init__.py:90\u001b[0m, in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m     89\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m---> 90\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _bootstrap\u001b[38;5;241m.\u001b[39m_gcd_import(name[level:], package, level)\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1387\u001b[0m, in \u001b[0;36m_gcd_import\u001b[1;34m(name, package, level)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1360\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1310\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:488\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[1;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1387\u001b[0m, in \u001b[0;36m_gcd_import\u001b[1;34m(name, package, level)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1360\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1324\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'langchain_community'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdocument_loaders\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m JSONLoader\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext_splitter\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RecursiveCharacterTextSplitter\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmetadata_func\u001b[39m(record: \u001b[38;5;28mdict\u001b[39m, metadata: \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mdict\u001b[39m:\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;66;03m#Populate the metadata dictionary with keys name, summary, url, category, and updated_at.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Huawei\\anaconda3\\Lib\\site-packages\\langchain\\document_loaders\\__init__.py:379\u001b[0m, in \u001b[0;36m__getattr__\u001b[1;34m(name)\u001b[0m\n\u001b[0;32m    377\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getattr__\u001b[39m(name: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    378\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Look up attributes dynamically.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 379\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _import_attribute(name)\n",
      "File \u001b[1;32mc:\\Users\\Huawei\\anaconda3\\Lib\\site-packages\\langchain\\_api\\module_import.py:72\u001b[0m, in \u001b[0;36mcreate_importer.<locals>.import_by_name\u001b[1;34m(name)\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     71\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m new_module\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlangchain_community\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m---> 72\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m(\n\u001b[0;32m     73\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModule \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnew_module\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     74\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease install langchain-community to access this module. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     75\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou can install it using `pip install -U langchain-community`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     76\u001b[0m         ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m     77\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: Module langchain_community.document_loaders not found. Please install langchain-community to access this module. You can install it using `pip install -U langchain-community`"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import JSONLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "\n",
    "def metadata_func(record: dict, metadata: dict) -> dict:\n",
    "    #Populate the metadata dictionary with keys name, summary, url, category, and updated_at.\n",
    "    None\n",
    "\n",
    "    return metadata\n",
    "\n",
    "\n",
    "# For more loaders https://python.langchain.com/docs/modules/data_connection/document_loaders/\n",
    "# And 3rd party loaders https://python.langchain.com/docs/modules/data_connection/document_loaders/#third-party-loaders\n",
    "loader = JSONLoader(\n",
    "    file_path=\"temp.json\",\n",
    "    jq_schema=\".[]\",\n",
    "    content_key=\"content\",\n",
    "    metadata_func=metadata_func,\n",
    ")\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=None, chunk_overlap=None #define chunk size and chunk overlap\n",
    ")\n",
    "docs = loader.load_and_split(text_splitter=text_splitter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bulk Import Passages\n",
    "\n",
    "Now that we have split each document into the chunk size of 800, we will now index data to elasticsearch using [ElasticsearchStore.from_documents](https://api.python.langchain.com/en/latest/vectorstores/langchain.vectorstores.elasticsearch.ElasticsearchStore.html#langchain.vectorstores.elasticsearch.ElasticsearchStore.from_documents).\n",
    "\n",
    "We will use Cloud ID, Password and Index name values set in the `Create cloud deployment` step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vectorstore' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m documents \u001b[38;5;241m=\u001b[39m vectorstore\u001b[38;5;241m.\u001b[39mfrom_documents(\n\u001b[0;32m      2\u001b[0m     docs,\n\u001b[0;32m      3\u001b[0m     embeddings,\n\u001b[0;32m      4\u001b[0m     index_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m      5\u001b[0m     es_cloud_id\u001b[38;5;241m=\u001b[39mELASTIC_CLOUD_ID,\n\u001b[0;32m      6\u001b[0m     es_api_key\u001b[38;5;241m=\u001b[39mELASTIC_API_KEY,\n\u001b[0;32m      7\u001b[0m )\n\u001b[0;32m      9\u001b[0m llm \u001b[38;5;241m=\u001b[39m OpenAI(temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, openai_api_key\u001b[38;5;241m=\u001b[39mOPENAI_API_KEY)\n\u001b[0;32m     11\u001b[0m retriever \u001b[38;5;241m=\u001b[39m MultiQueryRetriever\u001b[38;5;241m.\u001b[39mfrom_llm(vectorstore\u001b[38;5;241m.\u001b[39mas_retriever(), llm)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'vectorstore' is not defined"
     ]
    }
   ],
   "source": [
    "documents = vectorstore.from_documents(\n",
    "    docs,\n",
    "    embeddings,\n",
    "    index_name=None,\n",
    "    es_cloud_id=ELASTIC_CLOUD_ID,\n",
    "    es_api_key=ELASTIC_API_KEY,\n",
    ")\n",
    "\n",
    "llm = OpenAI(temperature=0, openai_api_key=OPENAI_API_KEY)\n",
    "\n",
    "retriever = MultiQueryRetriever.from_llm(vectorstore.as_retriever(), llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question Answering with MultiQuery Retriever\n",
    "\n",
    "Now that we have the passages stored in Elasticsearch, we can now ask a question to get the relevant passages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'retriever' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 37\u001b[0m\n\u001b[0;32m     32\u001b[0m     doc_strings \u001b[38;5;241m=\u001b[39m [format_document(doc, document_prompt) \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m docs]\n\u001b[0;32m     33\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m document_separator\u001b[38;5;241m.\u001b[39mjoin(doc_strings)\n\u001b[0;32m     36\u001b[0m _context \u001b[38;5;241m=\u001b[39m RunnableParallel(\n\u001b[1;32m---> 37\u001b[0m     context\u001b[38;5;241m=\u001b[39mretriever \u001b[38;5;241m|\u001b[39m _combine_documents,\n\u001b[0;32m     38\u001b[0m     question\u001b[38;5;241m=\u001b[39mRunnablePassthrough(),\n\u001b[0;32m     39\u001b[0m )\n\u001b[0;32m     41\u001b[0m chain \u001b[38;5;241m=\u001b[39m _context \u001b[38;5;241m|\u001b[39m LLM_CONTEXT_PROMPT \u001b[38;5;241m|\u001b[39m llm\n\u001b[0;32m     43\u001b[0m ans \u001b[38;5;241m=\u001b[39m chain\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhat is the nasa sales team?\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'retriever' is not defined"
     ]
    }
   ],
   "source": [
    "from langchain.schema.runnable import RunnableParallel, RunnablePassthrough\n",
    "from langchain.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain.schema import format_document\n",
    "\n",
    "import logging\n",
    "\n",
    "logging.basicConfig()\n",
    "logging.getLogger(\"langchain.retrievers.multi_query\").setLevel(logging.INFO)\n",
    "\n",
    "LLM_CONTEXT_PROMPT = ChatPromptTemplate.from_template(\n",
    "    \"\"\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Be as verbose and educational in your response as possible. \n",
    "    \n",
    "    context: {context}\n",
    "    Question: \"{question}\"\n",
    "    Answer:\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "LLM_DOCUMENT_PROMPT = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "---\n",
    "SOURCE: {name}\n",
    "{page_content}\n",
    "---\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "\n",
    "def _combine_documents(\n",
    "    docs, document_prompt=LLM_DOCUMENT_PROMPT, document_separator=\"\\n\\n\"\n",
    "):\n",
    "    doc_strings = [format_document(doc, document_prompt) for doc in docs]\n",
    "    return document_separator.join(doc_strings)\n",
    "\n",
    "\n",
    "_context = RunnableParallel(\n",
    "    context=retriever | _combine_documents,\n",
    "    question=RunnablePassthrough(),\n",
    ")\n",
    "\n",
    "chain = _context | LLM_CONTEXT_PROMPT | llm\n",
    "\n",
    "ans = chain.invoke(\"what is the nasa sales team?\")\n",
    "\n",
    "print(\"---- Answer ----\")\n",
    "print(ans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Generate at least two new iteratioins of the previous cells - Be creative.** Did you master Multi-\n",
    "Query Retriever concepts through this lab?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains import create_retrieval_chain\n",
    "\n",
    "# Enhanced prompt template\n",
    "ENHANCED_PROMPT = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"You are an expert workplace assistant. Answer questions using only the provided context. \n",
    "     If you don't know the answer, say you don't know. Provide detailed, accurate responses with examples when possible.\n",
    "     Context: {context}\"\"\"),\n",
    "    (\"user\", \"Question: {input}\")\n",
    "])\n",
    "\n",
    "# Create document chain\n",
    "document_chain = create_stuff_documents_chain(llm, ENHANCED_PROMPT)\n",
    "\n",
    "# Create retrieval chain\n",
    "retrieval_chain = create_retrieval_chain(retriever, document_chain)\n",
    "\n",
    "# Test the enhanced chain\n",
    "response = retrieval_chain.invoke({\"input\": \"What are the key responsibilities of the NASA sales team?\"})\n",
    "print(\"### Enhanced Response ###\")\n",
    "print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "# Create memory for conversation\n",
    "memory = ConversationBufferMemory(\n",
    "    memory_key=\"chat_history\",\n",
    "    return_messages=True\n",
    ")\n",
    "\n",
    "# Create conversational chain\n",
    "conversational_chain = ConversationalRetrievalChain.from_llm(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    memory=memory,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Test multi-turn conversation\n",
    "print(\"### Conversation 1 ###\")\n",
    "result1 = conversational_chain.invoke({\"question\": \"Tell me about the NASA sales team structure\"})\n",
    "print(result1[\"answer\"])\n",
    "\n",
    "print(\"\\n### Follow-up Question ###\")\n",
    "result2 = conversational_chain.invoke({\"question\": \"Who leads the North American division?\"})\n",
    "print(result2[\"answer\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
